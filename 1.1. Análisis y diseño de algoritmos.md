## Definicion
- Un algoritmo es un conjunto ordenado y finito de operaciones que permite hallar la solución a un problema.
-  **Mide la cantidad de recursos (tiempo y espacio)** que un algoritmo consume en función del tamaño de la entrada. Usamos notaciones como la Big O para expresar estos límites.
- **Técnicas comunes de diseño:Divide y vencerás, Programación Dinámica, Algoritmos Voraces.
- **Ejemplos clave:** Algoritmos de ordenación (burbuja, quicksort) y búsqueda (búsqueda binaria).
--------------------------------------------------------------------------
**Notación Big O:** Es una notación que describe el peor escenario posible en términos de tiempo o espacio. Por ejemplo, un algoritmo con una complejidad temporal de O(n) significa que, en el peor de los casos, el tiempo que tardará en ejecutarse es proporcional al tamaño de la entrada ^ff213c

## Para analizar un algoritmo, se consideran los siguientes pasos:

1. **Definir el problema:** Comprender claramente el problema a resolver.
2. **Diseñar el algoritmo:** Crear un conjunto de pasos para resolver el problema.
3. **Implementar el algoritmo:** Codificarlo en un lenguaje de programación.
4. **Analizar el algoritmo:** Evaluar su complejidad temporal y espacial.


### **Ejercicio Práctico:**

Considera el siguiente algoritmo simple: encontrar el elemento más grande en un arreglo de números.

![[Pasted image 20231027174644.png]]

El algoritmo encuentra el numero mas grande de una lista de numeros, lo almacena y muestra, esto en base a un recorrido de un for y un condicional.

--------------------------------------------------------------------------

### **Complejidad Algorítmica**

La **complejidad algorítmica** se refiere a cómo se comporta un algoritmo en términos de tiempo y/o espacio a medida que el tamaño de su entrada crece. Esta nos permite evaluar la eficiencia de un algoritmo. ^34b740

#### **Tipos de Complejidad**

1. **Complejidad Temporal (Tiempo)**: Mide la cantidad de tiempo que un algoritmo tarda en ejecutarse en función del tamaño de la entrada.
2. **Complejidad Espacial (Espacio)**: Mide la cantidad de memoria que un algoritmo utiliza en función del tamaño de la entrada.

#### **Notación Big O**

- **O(1)**: Tiempo constante. Sin importar el tamaño de la entrada, el algoritmo siempre tardará el mismo tiempo.
- **O(log n)**: Tiempo logarítmico. Común en algoritmos que dividen a la entrada por la mitad en cada paso, como la búsqueda binaria.
- **O(n)**: Tiempo lineal. El tiempo que tarda el algoritmo crece linealmente con el tamaño de la entrada, como la búsqueda secuencial.
- **O(n log n)**: Tiempo log-lineal. Común en algoritmos eficientes de ordenación, como merge sort o quicksort.
- **O(n^2)**, **O(n^3)**, ...: Tiempo polinomial. Se presentan en algoritmos con bucles anidados.
- **O(2^n)**: Tiempo exponencial. Común en algoritmos que resuelven problemas al explorar muchos subproblemas, como el problema del viajante.

#### **Análisis de Complejidad**

Para analizar la complejidad de un algoritmo:

1. **Ignorar Operaciones Constantes**: Por ejemplo, si un algoritmo tiene 3 operaciones que toman tiempo constante y un bucle que toma tiempo n, su complejidad será O(n).
2. **Considerar el Peor Caso**: Analizar qué tan mal podría ser la situación.
3. **Identificar Bucles**: Por cada bucle, la complejidad subirá por un factor de n.

### Notaciones de complejidad

Las notaciones más comunes para expresar la complejidad temporal son:

- **O**: Notación Big O. Representa el peor caso.
- **Ω**: Notación Omega. Representa el mejor caso.
- **Θ**: Notación Theta. Representa el caso promedio.

### Ejemplos de ejercicios de complejidad

### O(1): Tiempo constante
```python
def obtener_primer_elemento(lista):
    return lista[0]  # Acceso directo al primer elemento
```
**Explicación:** No importa el tamaño de `lista`, siempre toma el mismo tiempo acceder al primer elemento. La operación de acceso a un índice en una lista (o arreglo) es una operación de tiempo constante.

### O(log n): Tiempo logarítmico
```python
def busqueda_binaria(arreglo, objetivo):
    izquierda, derecha = 0, len(arreglo) - 1
    while izquierda <= derecha:
        medio = (izquierda + derecha) // 2
        if arreglo[medio] == objetivo:
            return medio
        elif arreglo[medio] < objetivo:
            izquierda = medio + 1
        else:
            derecha = medio - 1
    return -1
```
**Explicación:** En cada iteración del bucle `while`, el tamaño del problema se reduce a la mitad (ya sea la parte izquierda o la derecha del arreglo), por lo que el número total de operaciones necesarias crece logarítmicamente con el tamaño del arreglo.

### O(n): Tiempo lineal
```python
def busqueda_secuencial(lista, objetivo):
    for indice, elemento in enumerate(lista):
        if elemento == objetivo:
            return indice
    return -1
```
**Explicación:** Este bucle `for` recorre cada elemento de `lista` una vez, por lo que el tiempo de ejecución es directamente proporcional al número de elementos en `lista`.

### O(n log n): Tiempo log-lineal
```python
def quicksort(arreglo):
    if len(arreglo) <= 1:
        return arreglo
    else:
        pivote = arreglo[len(arreglo) // 2]
        menores = [x for x in arreglo if x < pivote]
        mayores = [x for x in arreglo if x > pivote]
        return quicksort(menores) + [pivote] + quicksort(mayores)
```
**Explicación:** Quicksort divide el arreglo en cada llamada recursiva (comportamiento logarítmico) y luego realiza el proceso de combinación que tiene un costo lineal, resultando en una complejidad general de O(n log n).

### O(n^2): Tiempo cuadrático
```python
def ordenamiento_burbuja(arreglo):
    n = len(arreglo)
    for i in range(n):
        for j in range(0, n-i-1):
            if arreglo[j] > arreglo[j+1]:
                arreglo[j], arreglo[j+1] = arreglo[j+1], arreglo[j]
    return arreglo
```
**Explicación:** Hay dos bucles anidados que iteran sobre el arreglo, y en el peor de los casos, cada uno de ellos recorre `n` elementos, lo que resulta en O(n * n) = O(n^2).

### O(2^n): Tiempo exponencial
```python
def fibonacci_recursivo(n):
    if n <= 1:
        return n
    else:
        return fibonacci_recursivo(n-1) + fibonacci_recursivo(n-2)
```
**Explicación:** La función `fibonacci_recursivo` se llama a sí misma dos veces por cada llamada, creando un árbol de recursión que crece exponencialmente con `n`. Esto se debe a que para calcular `fibonacci_recursivo(n)`, primero se deben calcular `fibonacci_recursivo(n-1)` y `fibonacci_recursivo(n-2)`, y así sucesivamente.

Estos ejemplos ilustran cómo se determina la complejidad de tiempo de diferentes algoritmos basados en su estructura y la forma en que procesan los datos.